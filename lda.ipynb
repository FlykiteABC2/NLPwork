{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import jieba\n",
    "import random\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "base_path = r\"D:\\\\Time_base\\\\NLPwork\\\\work2\"  # 修改为实际路径\n",
    "    \n",
    "    # 获取目录下所有txt文件\n",
    "novel_files = [\n",
    "        os.path.join(base_path, fname) for fname in os.listdir(base_path) if fname.endswith('.txt')\n",
    "    ]\n",
    "    \n",
    "    # 实验参数\n",
    "K_values = [20, 100, 500, 1000, 3000 ]  # 段落 token 数\n",
    "T_values = [ 8,24,48]                 # LDA 主题数\n",
    "units = ['word', 'char']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tokenize(input_text, segmentation_mode='word'):\n",
    "    punctuation_chars = set(string.punctuation + '（）【】…—\\n\\r\\t ')\n",
    "    unwanted_phrases = ['www', 'cr173', 'com', '下载站', '电子书', '免费', 'txt', '.']\n",
    "    \n",
    "    processed_text = input_text\n",
    "    for phrase in unwanted_phrases:\n",
    "        processed_text = processed_text.replace(phrase, '')\n",
    "    \n",
    "    result_tokens = []\n",
    "    if segmentation_mode == 'word':\n",
    "        segmented_words = jieba.lcut(processed_text)\n",
    "        result_tokens = [w for w in segmented_words if w not in punctuation_chars]\n",
    "    elif segmentation_mode == 'char':\n",
    "        result_tokens = [c for c in processed_text \n",
    "                        if c not in punctuation_chars \n",
    "                        and c.strip() \n",
    "                        and '\\u4e00' <= c <= '\\u9fff']\n",
    "    \n",
    "    return result_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples_from_files(file_list, segment_length, text_unit='word', sample_count=1000):\n",
    "    sample_data = []\n",
    "    label_list = []\n",
    "    num_files = len(file_list)\n",
    "    base_sample_per_file = sample_count // num_files\n",
    "    remainder_samples = sample_count % num_files\n",
    "\n",
    "    for idx, filename in enumerate(file_list):\n",
    "        # File reading with multiple encoding attempts\n",
    "        file_content = None\n",
    "        for encoding in ['utf-8', 'gb18030']:\n",
    "            try:\n",
    "                with open(filename, 'r', encoding=encoding) as file_obj:\n",
    "                    file_content = file_obj.read()\n",
    "                break\n",
    "            except Exception as read_error:\n",
    "                if encoding == 'gb18030':\n",
    "                    print(f\"Failed to read {filename}: {read_error}\")\n",
    "                continue\n",
    "\n",
    "        if file_content is None:\n",
    "            continue\n",
    "\n",
    "        # Token processing\n",
    "        token_list = clean_and_tokenize(file_content, text_unit)\n",
    "        if len(token_list) < segment_length:\n",
    "            print(f\"Insufficient tokens in {filename} (needed: {segment_length})\")\n",
    "            continue\n",
    "\n",
    "        # Sample calculation\n",
    "        required_samples = base_sample_per_file + (1 if idx < remainder_samples else 0)\n",
    "\n",
    "        # Segment extraction\n",
    "        extracted_segments = []\n",
    "        sampling_interval = max(1, (len(token_list) - segment_length) // required_samples)\n",
    "        \n",
    "        for pos in range(0, len(token_list) - segment_length + 1, sampling_interval):\n",
    "            current_segment = ' '.join(token_list[pos:pos + segment_length])\n",
    "            if current_segment.strip():\n",
    "                extracted_segments.append(current_segment)\n",
    "                if len(extracted_segments) >= required_samples:\n",
    "                    break\n",
    "\n",
    "        # Handle insufficient segments\n",
    "        while len(extracted_segments) < required_samples and extracted_segments:\n",
    "            extracted_segments.append(random.choice(extracted_segments))\n",
    "\n",
    "        # Update collections\n",
    "        sample_data.extend(extracted_segments)\n",
    "        base_name = os.path.basename(filename).replace('.txt', '')\n",
    "        label_list.extend([base_name] * len(extracted_segments))\n",
    "\n",
    "    return sample_data, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVC  # 添加这行代码\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置全局字体为支持中文的字体\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # Windows 系统\n",
    "# plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']  # macOS 系统\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "def evaluate_lda_rbf(X_topics, labels, n_splits=10, test_size=100):\n",
    "    \n",
    "    scores = []\n",
    "    # 初始化交叉验证对象\n",
    "    cv = ShuffleSplit(n_splits=n_splits, test_size=test_size, random_state=42)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # 进行交叉验证\n",
    "    for train_idx, test_idx in cv.split(X_topics):\n",
    "        # 初始化高斯核 SVM 分类器\n",
    "        clf = SVC(kernel='rbf', random_state=42)\n",
    "        # 训练分类器\n",
    "        clf.fit(X_topics[train_idx], labels[train_idx])\n",
    "        # 预测测试集\n",
    "        preds = clf.predict(X_topics[test_idx])\n",
    "        # 计算准确率并存储\n",
    "        scores.append(accuracy_score(labels[test_idx], preds))\n",
    "\n",
    "    # 返回平均准确率和标准差\n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(novel_files, K_values, T_values, units, total=1000, n_splits=10, test_size=100):\n",
    "    \n",
    "    # 获取目录下所有txt文件\n",
    "   \n",
    "\n",
    "    results = []\n",
    "    acc_data = {}\n",
    "\n",
    "    for unit in units:\n",
    "        for K in K_values:\n",
    "            print(f\"\\nUNIT {unit},LENGTH K={K}\")\n",
    "            dataset, labels = create_samples_from_files(novel_files, K, unit, total=total)\n",
    "\n",
    "            # 打印前 5 个样本作为案例\n",
    "            novel_count = {}\n",
    "            for label in labels:\n",
    "                novel_count[label] = novel_count.get(label, 0) + 1\n",
    "            print(\"段落数量：\", novel_count)\n",
    "\n",
    "            print(\"样例段落及其标签：\")\n",
    "            for i in range(5):\n",
    "                print(f\"标签：{labels[i]}\")\n",
    "                print(f\"内容：{dataset[i]}\")\n",
    "                print(\"=\" * 40)\n",
    "\n",
    "            if not dataset or len(dataset) < total:\n",
    "                print(f\"警告：生成的样本数量不足，当前样本数为 {len(dataset)}\")\n",
    "                continue\n",
    "\n",
    "            # 特征提取\n",
    "            if unit == 'char':\n",
    "                vectorizer = CountVectorizer(token_pattern=r'(?u)\\b\\w+\\b')\n",
    "            else:\n",
    "                vectorizer = CountVectorizer()\n",
    "            X = vectorizer.fit_transform(dataset)\n",
    "            print(f\"CountVectorizer 特征矩阵大小：{X.shape}\")\n",
    "\n",
    "            for T in T_values:\n",
    "                # 训练 LDA 模型\n",
    "                lda = LatentDirichletAllocation(n_components=T, random_state=42)\n",
    "                X_topics = lda.fit_transform(X)\n",
    "\n",
    "                # 评估分类性能\n",
    "                mean_acc, std_acc = evaluate_lda_rbf(X_topics, labels, n_splits=n_splits, test_size=test_size)\n",
    "                results.append((unit, K, T, mean_acc, std_acc))\n",
    "\n",
    "                # 存储结果到结构化字典\n",
    "                if unit not in acc_data:\n",
    "                    acc_data[unit] = {}\n",
    "                if K not in acc_data[unit]:\n",
    "                    acc_data[unit][K] = {}\n",
    "                acc_data[unit][K][T] = mean_acc\n",
    "\n",
    "                print(f\" K: {K}, T: {T} ACC {mean_acc:.4f}\")\n",
    "\n",
    "    # 绘制实验结果图\n",
    "    for current_unit in units:\n",
    "        plt.figure(figsize=(9, 7))\n",
    "        sorted_K_list = sorted(acc_data.get(current_unit, {}).keys())\n",
    "\n",
    "        for K_val in sorted_K_list:\n",
    "            mean_acc_list = [acc_data[current_unit][K_val].get(T_val, 0) for T_val in T_values]\n",
    "            plt.plot(T_values, mean_acc_list, marker='D', label=f'K={K_val}',linestyle=\"--\")\n",
    "\n",
    "        # 设置标题、坐标轴和图例\n",
    "        plt.title(f\"Acc unit='{current_unit}'\")\n",
    "        plt.xlabel(\"主题数量\")\n",
    "        plt.ylabel(\"分类准确率\")\n",
    "        plt.ylim([0, 1.0])\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return results, acc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "base_path = r\"D:\\\\Time_base\\\\NLPwork\\\\work2\"  # 修改为实际路径\n",
    "    \n",
    "    # 获取目录下所有txt文件\n",
    "novel_files = [\n",
    "        os.path.join(base_path, fname) for fname in os.listdir(base_path) if fname.endswith('.txt')\n",
    "    ]\n",
    "    \n",
    "    # 实验参数\n",
    "K_values = [20, 100, 500, 1000, 3000 ]  # 段落 token 数\n",
    "T_values = [ 8,24,48]                 # LDA 主题数\n",
    "units = ['word', 'char']  \n",
    "results, acc_data = run_experiment(novel_files, K_values, T_values, units)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
